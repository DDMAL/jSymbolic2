<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<title>Feature Explanations</title>
</head>

<body text="#000000" bgcolor="#e4e4e4" link="#0033C4" vlink="#0033C4" alink="#0033C4">
<a name="Top"></a>

<table cellspacing="0" cellpadding="4" width="100%" bgcolor="#0033C4" border="0">
  <tbody><tr>
      <th align="left"><font face="Arial" color="#ffffff" size="+3">Feature Explanations</font></th>
    </tr></tbody>
</table>

<p><strong>EXPLANATION OF THIS SECTION</strong></p>

<p>This section describes all of the features that jSymbolic extracts. It is drawn largely from Chapter 4 of <a href="http://www.music.mcgill.ca/~cmckay/papers/musictech/mckay10dissertation.pdf" target="top">Cory McKay's dissertation</a>, although several improvements and clarifications have been made, and certain new features have been added.</p>

<p>Note that, although many of the feature descriptions below use MIDI-specific terminology, they are  all compatible with alternative formats that jSymbolic can extract features from, such as MEI. Part of what jSybmolic does when encountering MEI files is to convert them to MIDI data in a very rich way, such that all the MIDI-related features can be extracted from them. MEI data that cannot be encoded as MIDI is maintained in a separate data pipeline, and features can be extracted from this pipeline as well (see the section below on MEI-specific features).</p>
<p>If a feature cannot be calculated for whatever reason for a given piece of music, the default behavior is to save a value of -1 for one-dimensional features, and null for multi-dimensional features.</p>

<p><strong>FEATURES BASED ON OVERALL PITCH STATISTICS</strong></p>
<p>The majority of traditional analytical systems place a particular emphasis on information related to pitch and, as one would expect, this type of information certainly has important value with respect to symbolic features as well. This first set of pitch-related features focuses on overall statistics on the pitches present in a piece, without regard to the temporal location of notes in relation to one another (features that do, in contrast, account for the context of notes and their pitches are presented in other sections below).</p>
<p>One particularly useful approach to calculating statistical features is to construct histograms, which consist of a set of bins each indicating some measurable quantity (e.g. how often each possible pitch occurs in a piece), and where the magnitude (or &quot;frequency&quot;) of each bin indicates how often that quantity occurs in the piece. The jSymbolic feature catalogue uses  modified versions of the three pitch histograms implemented by Tzanetakis and his colleagues (Tzanetakis and Cook 2002; Tzanetakis, Ermolinskyi and Cook 2002; Tzanetakis 2002).</p>
<p>The first type of histogram is a <strong>basic pitch histogram</strong>. It consists of 128 bins, one for each MIDI pitch. The magnitude of each bin is first set to the number of Note On messages in the piece with the corresponding pitch, and the histogram is normalized after all Note On messages have been accounted for. This type of histogram gives particular insights into the range and variety of pitches used in a piece.</p>
<p>To provide practical examples, the first figure below shows the basic pitch histogram for a Duke Ellington jazz piece, and the second shows the histogram for a Dr. Dre rap song. A number of genre-typical differences are immediately apparent from even a rough visual comparison of these two histograms, such as the fact that the rap song uses far fewer pitches than the jazz piece, for example.</p>
<p><img src="Sophisticated_Lady.gif"></p>
<p><img src="Forgot_About_Dre.gif"></p>
<p>The second type of histogram is called a <strong>pitch class histogram</strong>. It has one bin for each of the twelve pitch classes, which means that it is essentially a version of the basic pitch histogram where octaves are collapsed for each of the pitch classes. The magnitude of each bin is set to the number of Note On messages with a MIDI pitch that can be wrapped to this pitch class, with enharmonic equivalents assigned to the same pitch class number. The histogram is normalized, and the bins are translated so that the first bin corresponds to the pitch class with the highest magnitude, with the successive bins ordered chromatically in semitone increments. This type of histogram provides insights into areas such as the types of scales used and the amount of transposition that is present, for example.</p>
<p>The third type of histogram is called a <strong>folded fifths pitch class histogram</strong>, and is derived directly from the pitch class histogram. This histogram is calculated by reordering the bins of the original unordered pitch class histogram such that adjacent bins are separated by perfect fifths rather than semitones. This is done using the following equation:</p>
<p>B = (7a)mod(12)</p>
<p>where B is the folded fifths pitch histogram bin and a is the corresponding pitch class histogram bin. The number seven is used because this is the number of semitones in a perfect fifth, and the number twelve is used because there are twelve pitch classes in total. This histogram is useful for measuring dominant tonic relationships and for looking at types of transpositions.	</p>
<p>The utility of the folded fifths pitch histogram can be seen by comparing the first figure below, which shows the folded fifths pitch histogram for a Baroque Vivaldi concerto, with the second figure below, which shows the folded fifths pitch histogram for an atonal Schoenberg piano miniature. The Vivaldi piece never or rarely uses five of the twelve pitch classes, and the pitch classes that are used are clustered around one section of the circle of fifths. These are characteristics that one would typically expect of basic tonal music without many tonally distant modulations or significant use of chromaticism. In contrast, all of the pitch classes are used to a significant degree in the Schoenberg piece, and the most frequently used pitch classes are not clustered together on the circle of fifths, both of which are characteristics that one would expect of such an atonal piece.</p>
<p><img src="Four_Seasons.gif"></p>
<p><img src="Schoenberg.gif""></p>
<p>All three of these histogram types are included directly as features in the jSymbolic feature catalogue, and are also used to calculate a number of other features. </p>
<p>Part of the challenge of histogram-related features is that one must find a way to represent the information embedded in them as useful features. Although histograms like the three described above certainly can be used directly as feature vectors (and they certainly can be with jSymbolic), experience has shown that machine learning algorithms can sometimes have trouble learning to extract useful information from them in this raw form if they are too large. Feature histograms are, however, very useful in providing an intermediate data structure from which other features can be extracted. Experience has shown informally that the two highest peaks of  histograms tend to be of particular importance in extracting such information, and they are used as the basis for a number of features.</p>
<p>It should be noted that most of the jSymbolic features represent pitch as MIDI does, with 128 pitches (numbered 0 to 127), and with middle C set at note 60. A disadvantage with this approach, of course, is that enharmonic equivalents are assigned the same pitch number. Although this is not ideal, as it involves the loss of some potentially useful information, it is unavoidable given the need to be able to parse MIDI files and compare features extracted from MIDI files with features extracted from files in formats such as MEI. Fortunately, most of the jSymbolic features are not impacted by this loss of information. If one wishes to exclusively extract features from formats such as MEI that do distinguish between enharmonic equivalents, then jSymbolic can certainly still be used, although one will need to implement MEI-specific features (see below) that take advantage of jSymbolic's existing pipeline of MEI-specific data.</p>
<p>It should also be mentioned that all notes occurring on MIDI channel ten are ignored for all of the features described in this section. This is because the &quot;pitch&quot; values on channel ten correspond to (mostly unpitched) percussion instruments, not to pitches.</p>
<p>Some of the features in this section are based on MIDI Pitch Bends. Although the use of Pitch Bends is somewhat variable from MIDI encoder to MIDI encoder, and therefore not entirely dependant on the music itself, features relating to Pitch Bends can nonetheless have a high discriminating power, so they are included here. Efforts were made to use features with as limited a sensitivity to non-musical factors as possible.</p>
<p>The jSymbolic feature catalogue includes the following features related to overall pitch statistics:</p>
<ul>
  <li><strong>P-1 Basic Pitch Histogram:</strong> A feature vector consisting of bin magnitudes of the basic pitch  histogram described above. Each bin corresponds to one of the 128 MIDI pitches, ordered from lowest to highest, and with an interval of a semitone between each (enharmonic equivalents are assigned the same pitch  number). Bin 60 corresponds to middle C. The magnitude of of each bin is proportional to the the number of times notes occurred at the bin's pitch in the piece, relative to all other pitches in the piece (the histogram is normalized).</li>
  <li><strong>P-2 Pitch Class Histogram:</strong> A feature vector consisting of bin magnitudes of the pitch class histogram described above. Each bin corresponds to one of the 12 pitch classes, ordered in increasing pitch with an interval of a semitone between each (enharmonic equivalents are assigned the same pitch class number). The first bin corresponds to the most common pitch class in the piece under consideration (it does NOT correspond to a set pitch class). The magnitude of of each bin is proportional to the the number of times notes occurred at the bin's pitch class in the piece, relative to all other pitch classes in the piece (the histogram is normalized).</li>
  <li><strong>P-3 Folded Fifths Pitch Class Histogram:</strong> A feature vector consisting of bin magnitudes of the folded fifths pitch class histogram described above. Each bin corresponds to one of the 12 pitch classes, and the bins are ordered such that adjacent bins are separated by an ascending perfect fifth. Bin 0 corresponds to C. Enharmonic equivalents are assigned the same pitch class number. The magnitude of of each bin is proportional to the the number of times notes occurred at the bin's pitch class in the piece, relative to all other pitch classes in the piece (the histogram is normalized).</li>
  <li><strong>P-4 Prevalence of Most Common Pitch:</strong> Fraction of notes that correspond to the most common pitch.</li>
  <li><strong>P-5 Prevalence of Most Common Pitch Class:</strong> Fraction of notes that correspond to the most common pitch class.</li>
  <li><strong>P-6 Relative Prevalence of Top Pitches:</strong> Relative frequency of the  second most common pitch in the piece, divided by the relative frequency of the most common pitch.</li>
  <li><strong>P-7 Relative Prevalence of Top Pitch Classes:</strong> Relative frequency of the of the second most common pitch class in the piece, divided by the relative frequency of the most common pitch class.</li>
  <li><strong>P-8 Interval Between Most Prevalent Pitches:</strong> Absolute value of the difference (in semitones) between the pitches of the two most frequently occurring pitches.</li>
  <li><strong>P-9 Interval Between Most Prevalent  Pitch Classes:</strong> Absolute value of the difference (in semitones) between the pitches of the two most frequently occurring  pitch classes.</li>
  <li><strong>P-10 Number of Common Pitches:</strong> Number of pitches that account individually for at least 9% of all notes. Enharmonic equivalents are grouped together for the purpose of this calculation.</li>
  <li><strong>P-11 Pitch Variety:</strong> Number of pitches that occur at least once in the piece. Enharmonic equivalents are grouped together for the purpose of this calculation.</li>
  <li><strong>P-12 Pitch Class Variety:</strong> Number of pitch classes that occur at least once in the piece. Enharmonic equivalents are grouped together for the purpose of this calculation.</li>
  <li><strong>P-13 Range:</strong> Difference in semitones between the highest and lowest pitches.</li>
  <li><strong>P-14 Most Common Pitch:</strong> MIDI pitch value of the most frequently occurring pitch.</li>
  <li><strong>P-15 Mean Pitch:</strong> Mean MIDI pitch value, averaged across all pitched notes in the piece.  Set to 0 if there are no pitched notes.</li>
  <li><strong>P-16 Importance of Bass Register:</strong> Fraction of notes between MIDI pitches 0 and 54.</li>
  <li><strong>P-17 Importance of Middle Register:</strong> Fraction of notes between MIDI pitches 55 and 72.</li>
  <li><strong>P-18 Importance of High Register:</strong> Fraction of notes between MIDI pitches 73 and 127.</li>
  <li><strong>P-19 Most Common Pitch Class:</strong> The pitch class that occurs most frequently compared to other pitch classes. A value of 0 corresponds to C, and pitches increase chromatically by semitone in integer units (e.g. a value of 2 would mean that D is the most common pitch class). Enharmonic equivalents are treated as a single pitch class.</li>
  <li><strong>P-20 Dominant Spread:</strong> Largest number of consecutive pitch classes separated by perfect 5ths that each individually account for at least 9% of the total notes in the piece.</li>
  <li><strong>P-21 Strong Tonal Centres:</strong> Number of isolated peaks in the fifths pitch histogram that each individually account for at least 9% of all notes in the piece.</li>
  <li><strong>P-22 Major or Minor:</strong> Whether the piece is major or minor, as indicated by the first encountered major/minor metadata tag in the piece. Set to 0 if the metadata indicates that the piece is major, or set to 1 if the metadata indicates that it is minor. Defaults to 0 if the key signature is unknown.</li>
  <li><strong>P-23 Glissando Prevalence:</strong> Number of pitched MIDI Note Ons that have at least one MIDI Pitch Bend associated with them, divided by the total number of pitched Note Ons in the piece.</li>
  <li><strong>P-24 Average Range of Glissandos:</strong> Average range of MIDI Pitch Bends, where &quot;range&quot; is defined as the greatest value of the absolute difference between 64 and the second data byte of all MIDI Pitch Bend messages falling between the Note On and Note Off messages of any note in the piece. Set to 0 if there are no MIDI Pitch Bends in the piece.</li>
  <li><strong>P-25 Vibrato Prevalence:</strong> Number of pitched notes  that have associated MIDI Pitch Bend messages change direction at least twice in connection with the note in question, divided by the total number of pitched Note Ons in the piece.</li>
  <li><strong>P-26  Microtone Prevalence:</strong> Number of pitched notes that are  each associated with exactly one MIDI Pitch Bend message, divided by the total number of pitched Note Ons in the piece. Set to 0 if there are no pitched Note Ons in the piece.</li>
</ul>
<p><strong>FEATURES BASED ON MELODIC INTERVALS</strong></p>
<p>Although features based on overall pitch statistics are often meaningful and useful, they do not reflect information relating to the order in which pitches occur. Melody is a very important part of how many humans hear and think about music, so features based on such sequential information are needed to complement features based on overall pitch statistics. Fortunately, ample theoretical work has been done that can be taken advantage of when designing melodic features, ranging from compositional resources like manuals on writing Baroque counterpoint, to more analytically formulated ideas like melodic contour.</p>
<p>Unfortunately, the tasks of detecting and partitioning musical phrases and melodies, and of determining which notes belong to which phrases, are not trivial. Although expert humans can perform such tasks relatively easily, automatic systems for performing them have still achieved only limited general success, particularly in cases where the notes in a phrases are shared across voices. So, although a phrase detection pre-processing system would make many potentially useful melodic features accessible, such a system is not currently available.</p>
<p>What one can do fairly easily, however, is collect basic statistics about melodic intervals and melodic motion. Although such statistics may be relatively rudimentary compared to expert melodic analyses, they can still potentially be very effective in performing classifications. One can also extract somewhat more sophisticated features related to melodic contour by making a few naive but often effective basic assumptions, such as the assumptions that all notes belonging to a phrase will be on the same MIDI channel and that phrases will each follow the overall shape of a basic concave or converse arc. Although such assumptions are clearly false, and certainly not acceptable for any wide-ranging analytical framework, they do make it possible to extract some potentially discriminating higher-level melodic features without a sophisticated phrase detection system.</p>
<p>A <strong>melodic interval histogram</strong> is proposed here as a way of facilitating the extraction of certain basic features relating to melodic intervals. Each bin of this histogram represents a different melodic interval, and is labeled with an index indicating the number of semitones in the interval. The magnitude of each bin is set to the number of Note On messages in the piece that have a pitch interval from the preceding Note On message on the same MIDI channel corresponding to the bin label. The direction of the interval (i.e., up or down in pitch) is ignored in this histogram. The histogram is then normalized, so that the magnitude of each bin indicates the fraction of all melodic intervals that correspond to the melodic interval of the given bin.</p>
<p>This histogram clearly has a few limitations. It treats all voices equally, for example, even though the highest line of a piece often carries the most significant melodic information. It is also problematic for polyphonic instruments such as pianos that can play harmonies or multiple melodies simultaneously. It is, however, a quick and easy approach that has been found experimentally to often be helpful in discriminating between classes.</p>
<p>Another intermediate data structure is also used to help calculate some of the features listed below. This consists of a list of MIDI channels, where each entry of the list contains an array. Each array entry corresponds to a MIDI channel, and each entry in the array consists of a list of all melodic intervals, in semitones, for the associated channel. The numbers representing the intervals in this second intermediate data structure are set to negative for downward motion and to positive for upward motion.</p>
<p>Once again, all notes occurring on MIDI channel ten are ignored for all of the features described in this section. This is because the “pitch” values on channel ten correspond to percussion instruments, not to pitches.</p>
<p>The jSymbolic feature catalogue includes the following features related to melody and melodic intervals:</p>
<ul>
  <li><strong>M-1 Melodic Interval Histogram:</strong> A feature vector consisting of the bin magnitudes of the melodic interval histogram described above. Each bin corresponds to a melodic interval, and the bin index indicates the number of semitones comprising the interval associated with the bin (there are 128 bins in all). For example, bin 0 corresponds to repeated pitches, bin 1 to a melodic interval of one semitone, bin 2 to a melodic interval of 2 semitones, etc. The magnitude of each bin is proportional to the fraction of melodic intervals in the piece that are of the kind associated with the bin (this histogram is normalized). Rising and falling intervals are treated as identical. Melodies are assumed to be contained within individual MIDI tracks and channels, so melodic intervals are found separately for each track and channel before being combined in this histogram. It is also assumed that there is only one melody at a time per MIDI channel (if multiple notes occur simultaneously on the same MIDI tick on the same MIDI track and channel, then all notes but the first note on that tick are ignored). Other than this, all notes on the same track and the same channel are treated as if they are part of a single melody. It is also assumed that melodies do not cross MIDI tracks or channels (i.e. that they are each separately contained in their own track and channel). Only pitched notes are considered, so all notes on the unpitched MIDI Channel 10 are ignored.</li>
  <li><strong>M-2 Most Common Melodic Interval:</strong> Number of semitones corresponding to the most frequently occurring melodic interval.</li>
  <li><strong>M-3 Mean Melodic Interval:</strong> Mean average (in semitones) of the intervals involved in each of the melodic intervals in the piece.</li>
  <li><strong>M-4 Number of Common Melodic Intervals:</strong> Number of different melodic intervals that each account individually for at least 9% of all melodic intervals.</li>
  <li><strong>M-5 Distance Between Most Prevalent Melodic Intervals:</strong> Absolute value of the difference (in semitones) between the most common and second most common melodic intervals in the piece.</li>
  <li><strong>M-6 Prevalence of Most Common Melodic Interval:</strong> Fraction of all melodic intervals that corresponds to the most common melodic interval.</li>
  <li><strong>M-7 Relative Prevalence of Most Common Melodic Intervals:</strong> Relative frequency of the  second most common melodic interval in the piece, divided by the relative frequency of the most common melodic interval.</li>
  <li><strong>M-8 Amount of Arpeggiation:</strong> Fraction of melodic intervals that are repeated notes, minor thirds, major thirds, perfect fifths, minor sevenths, major sevenths, octaves, minor tenths or major tenths. This is only a very approximate measure of the amount of arpeggiation in the music, of course.</li>
  <li><strong>M-9 Repeated Notes:</strong> Fraction of melodic intervals that correspond to repeated notes.</li>
  <li><strong>M-10 Chromatic Motion:</strong> Fraction of melodic intervals that correspond to a semitone.</li>
  <li><strong>M-11 Stepwise Motion:</strong> Fraction of melodic intervals that correspond to a minor or major second.</li>
  <li><strong>M-12 Melodic Thirds:</strong> Fraction of melodic intervals that are major or minor thirds.</li>
  <li><strong>M-13 Melocid Perfect Fourths:</strong> Fraction of melodic intervals that are perfect fourths.</li>
  <li><strong>M-14 Melodic Tritones:</strong> Fraction of melodic intervals that are tritones.</li>
  <li><strong>M-15 Melodic Fifths:</strong> Fraction of melodic intervals that are perfect fifths.</li>
  <li><strong>M-16 Melodic Sixths:</strong> Fraction of melodic intervals that are major or minor sixths.</li>
  <li><strong>M-17 Melodic Sevenths:</strong> Fraction of melodic intervals that are major or minor sevenths.</li>
  <li><strong>M-18 Melodic Octaves:</strong> Fraction of melodic intervals that are octaves.</li>
  <li><strong>M-19 Melodic Large Intervals:</strong> Fraction of melodic intervals greater than one octave.</li>
  <li><strong>M-20 Minor Major Melodic Third Ratio:</strong> Combined fraction of all melodic intervals that are minor thirds, divided by the combined fraction of all melodic intervals that are major thirds. Set to 0 if there are no melodic minor thirds or melodic major thirds.</li>
  <li><strong>M-21 Melodic Embellishments:</strong> Fraction of all notes that are surrounded on both sides by MIDI Note Ons on the same MIDI channel that have durations at least three times as long as the central note. Set to 0 if there are no notes in the piece.</li>
  <li><strong>M-22 Direction of Melodic Motion:</strong> Fraction of melodic intervals that are rising  in pitch. Set to zero if no rising or falling melodic intervals are found.</li>
  <li><strong>M-23 Average Length of Melodic Arcs:</strong> Average number of notes that separate melodic peaks and troughs. Similar assumptions are made in the calculation of this feature as for the Melodic Interval Histogram.  Set to 0 if no melodic arcs are found.</li>
  <li><strong>M-24 Average Interval Spanned by Melodic Arcs:</strong> Average melodic interval (in semitones) separating the top note of melodic peaks and the bottom note of adjacent melodic troughs. Similar assumptions are made in the calculation of this feature as for the Melodic Interval Histogram.</li>
  <li><strong>M-25 Melodic Pitch Variety:</strong> Average number of notes that go by in a MIDI channel before a note's pitch is repeated. This is calculated across each channel individually before being combined. Notes that occur simultaneously on the same MIDI tick are only counted as one note for the purpose of this calculation. Notes that do not recur after 16 notes in the same channel are not included in this calculation.
 Set to 0 if there are no qualifying repeated notes in the piece.    </li>
</ul>
<p><strong>FEATURES BASED ON CHORDS AND VERTICAL INTERVALS</strong></p>
<p>Chords in general, and tonal harmony in particular, are the areas that have typically received the most attention in traditional Western analytical systems. It is essential to point out that the existing theoretical frameworks based on tonal harmony do not apply to many kinds of music, as they were developed primarily with respect to Western classical music. As a consequence, most of the chord-based features proposed as part of the jSymbolic feature library are not based on harmonic function, and emphasize instead basic statistical information about the vertical intervals between pitches that sound simultaneously. Having recognized this, it is once again important to recall that features are useful simply if they help to differentiate between classes statistically, even if they are inspired by theoretical assumptions that do not apply to some of the music under consideration. There are therefore certain features in the jSymbolic catalogue that make use of certain basic concepts that are specific to Western harmonic theory.</p>
<p>Two new histograms are proposed as intermediate data structures for calculating chord-based structures. The first, called a <strong>vertical interval histogram</strong>, consists of bins associated with different vertical intervals and labeled with the number of semitones in the corresponding interval. The magnitude of each bin is found by going through a recoding MIDI  tick by MIDI tick and noting all vertical intervals that are sounding at each tick, as well as the MIDI velocities of the pair of notes involved in each vertical interval. This is done exhaustively, so that multiple vertical intervals will be noted per tick if there are more than two pitches sounding simultaneously. The histogram is then normalized. The end result is a histogram that indicates which vertical intervals are present, and how significant these vertical intervals are relative to one another, with a weighting based on both MIDI velocity and the aggragated durations with which each interval is held throughout the piece. This is reasonable, since long notes often have greater harmonic significance than short notes, and stressed notes tend to be more harmonically important than notes played more softly. This histogram does not incorporate any tonal assumptions, although it does require quantization into the twelve standard Western pitches (i.e. microtones are not considered). Another histogram, the <strong>wrapped vertical interval histogram</strong>, is also found: it simply consists of the vertical interval histogram wrapped by octave, such that there are 12 bins.</p>
<p>
  It is also potentially useful to have a histogram that can be used to extract features more directly related to tonal harmony, since many music classification projects do involve music based on the basic chord ontologies of Western music. A <strong>chord type histogram</strong> is proposed with this need in mind. This is a normalized histogram that has bins labeled with types of chords (in the following order and with the indicated identifying codes): partial chords consisting of just two pitch classes [0], minor triads [1], major triads [2], diminished triads [3], augmented triads [4], other triads [5], minor seventh chords [6], dominant seventh chords [7], major seventh chords [8], other chords consisting of four pitch classes [9], and complex chords with more than four pitch classes [10]. The bin magnitudes are calculated by going through MIDI ticks one by one and incrementing the counter for the bin that corresponds to the chord, if any, that is present during each given tick; the result is that the chords in this histogram are weighted by the duration with which each chord is played. All inversions are treated as equivalent and octave doubling is ignored in the calculation of this histogram.</p>
<p>
  None of these histograms provide any information about arpeggiation, unfortunately, but some information related to this is collected during the melodic feature extraction. A more sophisticated system in the future could integrate vertical statistics with arpeggios, and could also collect information about inversions as well as chord transitions in order to obtain more sophisticated and accurate features. </p>
<p>
  Once again, all notes occurring on MIDI channel ten are ignored for all of the features described in this section. This is because the &quot;pitch&quot;   values on channel ten correspond to percussion instruments, not to pitches.</p>
<p>The jSymbolic feature catalogue includes the following features related to chords and vertical intervals:</p>
<ul>
  <li><strong>	C-1 Vertical Interval Histogram:</strong> A feature vector consisting of bin magnitudes of the vertical interval histogram described above. Each of the bins is associated with a different vertical pitch interval, and is labeled with the number of semitones in that corresponding interval. More specifically, these are numbered from 0 (a unison) to 127 (a vertical interval of 127 semitones). The magnitude of each bin is found by going through a recoding MIDI tick by MIDI tick and noting all vertical intervals that are sounding at each tick, as well as the MIDI velocities of the pair of notes involved in each vertical interval. The end result is a histogram that indicates which vertical intervals are present, and how significant these vertical intervals are relative to one another, with a weighting based on both MIDI velocity and the aggragated durations with which each interval is held throughout the piece. Finally, the histogram is normalized.</li>
  <li><strong>C-2 Wrapped Vertical Interval Histogram:</strong> A feature vector consisting of bin magnitudes of the wrapped vertical interval histogram described above. Each of the bins is associated with a different vertical pitch interval, and is labeled with the number of semitones in that corresponding interval. More specifically, these are numbered from 0 (a unison) to 11 (a vertical interval of 11 semitones). The magnitude of each bin is found by going through a recoding MIDI tick by MIDI tick and noting all vertical intervals that are sounding at each tick, as well as the MIDI velocities of the pair of notes involved in each vertical interval. Intervals larger than 11 semitones are wrapped (e.g. an octave (12 semitones) is added to the bin for unisons (0 semitones)). The end result is a histogram that indicates which vertical intervals are present, and how significant these vertical intervals are relative to one another, with a weighting based on both MIDI velocity and the aggregated durations with which each interval is held throughout the piece. Finally, the histogram is normalized.</li>
  <li><strong>C-3 Chord Type Histogram:</strong> A feature vector consisting of bin magnitudes of the chord type histogram described above. This is a normalized histogram that has bins labeled with types of chords (in the following order and with the indicated identifying codes): partial chords consisting of just two pitch classes [0], minor triads [1], major triads [2], diminished triads [3], augmented triads [4], other triads [5], minor seventh chords [6], dominant seventh chords [7], major seventh chords [8], other chords consisting of four pitch classes [9], and complex chords with more than four pitch classes [10]. The bin magnitudes are calculated by going through MIDI ticks one by one and incrementing the counter for the bin that corresponds to the chord, if any, that is present during each given tick; the result is that the chords in this histogram are weighted by the duration with which each chord is played. All inversions are treated as equivalent and octave doubling is ignored in the calculation of this histogram. Melodic behaviour is not considered, so arpeggios are not counted in this histogram.</li>
  <li><strong>C-4 Average Number of Simultaneous Pitch Classes:</strong> Average number of different pitch classes sounding simultaneously. Rests are excluded from this calculation.</li>
  <li><strong>C-5 Variability of Number of Simultaneous Pitch Classes:</strong> Standard deviation of the number of different pitch classes sounding simultaneously. Rests are excluded from this calculation.</li>
  <li><strong>C-6 Average Number of Simultaneous Pitches:</strong> Average number of pitches sounding simultaneously. Rests are excluded from this calculation. Unisons are also excluded from this calculation, but octave multiples are included in it.</li>
  <li><strong>C-7 Variability of Number of Simultaneous Pitches:</strong> Standard deviation of the number of pitches sounding simultaneously. Rests are excluded from this calculation. Unisons are also excluded from this calculation, but octave multiples are included in it.</li>
  <li><strong>C-8 Most Common Vertical Interval:</strong> The interval in semitones corresponding to the wrapped vertical interval histogram bin with the highest magnitude.</li>
  <li><strong>	C-9 Second Most Common Vertical Interval:</strong> The interval in semitones corresponding to the wrapped vertical interval histogram bin with the second highest magnitude.</li>
  <li><strong>C-10 Distance Between Two Most Common Vertical Intervals:</strong> The interval in semitones between the wrapped vertical interval histogram bins with the two most common vertical intervals.</li>
  <li><strong>C-11 Prevalence of Most Common Vertical Interval:</strong> Fraction of vertical intervals on the wrapped vertical interval histogram corresponding to the most common vertical interval.</li>
  <li><strong>C-12 Prevalence of Second Most Common Vertical Interval:</strong> Fraction of vertical intervals on the wrapped vertical interval histogram corresponding to the second most common vertical interval.</li>
  <li><strong>C-13 Prevalence Ratio of Two Most Common Vertical Intervals:</strong> Ratio between the fraction of notes corresponding to the second most common vertical interval on the wrapped vertical interval histogram and the fraction of vertical intervals corresponding to the most common vertical interval. Set to 0 if either of these prevalences are 0.</li>
  <li><strong>C-14 Vertical Unisons:</strong> Fraction of all  vertical intervals that are unisons. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li>
<strong>	C-15 Vertical Minor Seconds:</strong> Fraction of all wrapped vertical intervals that are minor seconds. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-16 Vertical Thirds:</strong> Fraction all wrapped vertical intervals that are minor or major thirds. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-17 Vertical Tritones:</strong> Fraction of all wrapped vertical intervals that are tritones. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-18 Vertical Perfect Fourths:</strong> Fraction of all wrapped vertical intervals that are perfect fourths. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-19 Vertical Perfect Fifths:</strong> Fraction of all wrapped vertical intervals that are perfect fifths. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-20 Vertical Sixths:</strong> Fraction all wrapped vertical intervals that are minor or major sixths. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-21 Vertical Sevenths:</strong> Fraction all wrapped vertical intervals that are minor or major sevenths. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-22 Vertical Octaves:</strong> Fraction of all wrapped vertical intervals that are octaves. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-23 Perfect Vertical Intervals:</strong> Fraction of all wrapped vertical intervals that are unisons, perfect fourths, perfect fifths or octaves. This is weighted by how long intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note).</li>
  <li><strong>C-24 Vertical Dissonance Ratio:</strong> Ratio of all wrapped vertical intervals that are dissonant (2nds, tritones, and 7ths to all wrapped vertical intervals that are consonant (unisons, 3rds, 4ths, 5ths, 6ths, octaves). This is weighted by how long each of these intervals are held (e.g. an interval lasting a whole note will be weighted four times as strongly as an interval lasting a quarter note). Set to 0 if there are no dissonant vertical intervals or no consonant vertical intervals.</li>
  <li><strong>C-25 Vertical Minor Third Prevalence:</strong> Fraction of the music by time where at least one wrapped vertical minor third is sounding (regardless of whatever other vertical intervals may or may not be sounding at the same time). Only that part of the music where one or more pitched notes is sounding is included in this calculation (rests and sections containing only unpitched notes are ignored).</li>
  <li><strong>C-26 Vertical Major Third Prevalence:</strong> Fraction of the music by time where at least one wrapped vertical major third is sounding (regardless of whatever other vertical intervals may or may not be sounding at the same time). Only that part of the music where one or more pitched notes is sounding is included in this calculation (rests and sections containing only unpitched notes are ignored).</li>
  <li><strong>C-27 Chord Duration:</strong> Average duration in seconds of a chord. A &quot;chord&quot; here is considered to stay the same as long as no new pitch classes are added, and no pitch classes are taken away. This &quot;chord&quot; may consist of any number of pitch classes, even only one. A &quot;chord&quot; is not considered to end if it is split by one or more rests (although the rests themselves are not counted in the duration of the &quot;chord&quot;).</li>
  <li><strong>C-28 Partial Chords:</strong> Fraction of simultaneously sounding pitch groups that consist of only two pitch classes. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-29 Standard Triads:</strong> Fraction of all simultaneously sounding pitch groups that are either major or minor triads. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-30 Diminished and Augmented Triads:</strong> Fraction of all simultaneously sounding pitch groups that are either diminished or augmented triads. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-31 Dominant Seventh Chords:</strong> Fraction of all simultaneously sounding pitch groups that are dominant seventh chords. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-32 Seventh Chords:</strong> Fraction of all simultaneously sounding pitch groups that are dominant seventh, major seventh or minor seventh chords. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-33 Non-Standard Chords:</strong> Fraction of all simultaneously sounding pitch groups that consist of more than two pitch classes yet are not major triads, are not minor triads and are not seventh chords. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note).</li>
  <li><strong>C-34 Complex Chords:</strong> Fraction of all simultaneously sounding pitch groups that contain more that four pitch classes. This is weighted by how long pitch groups are held (e.g. a pitch group lasting a whole note will be weighted four times as strongly as a pitch group lasting a quarter note). </li>
  <li><strong>C-35 Minor Major Triad Ratio:</strong> The prevalence of minor triads divided by the prevalence of major triads. This is weighted by how long the chords are held (e.g. a chord lasting a whole note will be weighted four times as strongly as a chord lasting a quarter note). Set to 0 if there are no minor triads or if there are no major triads.</li>
</ul>
<p><strong>FEATURES BASED ON RHYTHM</strong></p>
<p>The two elementary pieces of information from which most rhythmic features can be calculated are the times at which notes begin (called note onsets) relative to one another, and the durations of notes. Note onsets can be extracted relatively reliably from audio data, at least in cases where note density is not too high, but durations are more difficult to extract reliably. In the case of symbolic data, however, both note onsets and durations are easily and precisely available. As one might expect, several of the rhythmic features that are based on note onsets in the jSymbolic catalogue are very similar to features that are often used in audio feature extraction systems. Duration-based features, in contrast, are very rarely currently used by audio feature extraction software, but can easily be extracted from symbolic data, and are thus included in the jSymbolic feature catalogue in order to allow their utility to be empirically evaluated.</p>
<p>Before proceeding to discuss the details of the jSymbolic rhythmic feature catalogue, it is important to emphasize a detail of how MIDI encodes rhythmic information that must be considered when designing rhythmic features, whether for jSymbolic or for some other MIDI software. MIDI timings are affected by both the number of MIDI ticks that go by between Note On events and by tempo change meta-events that control the rate at which MIDI ticks go by. Tempo change meta-events must therefore be monitored by the feature extraction software, something which jSymbolic does of course do.</p>
<p>One disadvantage of symbolic data is that some important rhythmic features are related to performance characteristics that are not always available in symbolic data, or available in only a very coarse sense. For example, musical scores may indicate that a piece should be played rubato or with a swing rhythm. There is a great deal of variety in the ways in which these rhythmic performance styles can be implemented, however, something that can be of essential importance for tasks such as performer identification. Although formats such as MIDI certainly can represent precise note onset timings, and many recorded MIDI performances do indeed take advantage of this, MIDI files that are generated using score writing software are often strictly quantized, which means that performance timing information is not always consistently available with the precision that would ideally be preferred.</p>
<p>Nonetheless, even quantized rhythms can still result in very useful feature values. One of the nice things about MIDI is that it allows one to access timing information in terms of raw time of note onsets as well as in terms of rhythmic note values (i.e., half notes, quarter notes, etc.), thus providing both low-level and high-level rhythmic information. This information, along with time signature and tempo change meta-events, can potentially provide features with a high discriminating power.</p>
<p>Of course, as mentioned above, MIDI rhythmic information is somewhat sensitive to MIDI encoding style. This inconsistency is precisely the reason why the jSymbolic feature catalogue places a particular emphasis on rhythmic features derived from a <strong>beat histogram</strong>, as described below. This approach helps to statistically smooth over some inconsistencies due to encoding style.</p>
<p>Beat histograms are an approach that was first applied to MIR research by Brown (1993), and was later publicized and used for automatic genre classification by Tzanetakis and his colleagues in a number of papers (Tzanetakis, Essl &amp; Cook 2001; Tzanetakis &amp; Cook 2002; Tzanetakis 2002).</p>
<p>It is necessary to have some understanding of how autocorrelation works in order to understand how beat histograms are constructed. Autocorrelation essentially involves comparing a signal with versions of itself delayed by successive intervals. This technique is often used to find repeating patterns in signals of any kind, as it yields the relative strengths of different periodicities within a signal. In terms of musical data, autocorrelation allows one to find the relative strengths of different rhythmic pulses. jSymbolic constructs its rhythmic histograms by processing sequences of MIDI Note On events, with MIDI ticks comprising the time scale. The value is calculated to be proportional to the velocity of Note Ons in order to ensure that beats are weighted based on the strength with which notes are played.  The values of lag correspond to both rhythmic periodicities as well as, after processing, the bin labels of the beat histogram, and the autocorrelation values provide the magnitude value for each bin.</p>
<p>Once the histogram is populated using all permissible values of lag for a given MIDI sequence, jSymbolic then downsamples and transforms it so that each bin corresponds to a rhythmic periodicity with units of beats per minute. The histogram is then normalized so that different MIDI sequences can be compared. The end result is a histogram whose bins correspond to rhythmic pulses with units of beats per minute and whose bin magnitudes indicate the relative strength of each such rhythmic pulse. In effect, a beat histogram portrays the relative strength of different beats and sub-beats within a piece.</p>
<p>Consider, for example, the beat histograms extracted from MIDI representations of I Wanna Be Sedated, by the punk band The Ramones, and ’Round Midnight, by the jazz performer and composer Thelonious Monk, as shown below. It is clear that I Wanna Be Sedated has significant rhythmic looseness, as demonstrated by the spread around each peak, each of which represents a strong beat periodicity. I Wanna Be Sedated also has several clear strong beats, including ones centered at 55, 66, 82, 111 (the actual tempo of the song) and 164 beats per minute, the latter two of which are harmonics of 55 and 82 beats per minute. ’Round Midnight, in contrast, has one very strong beat at 76 beats per minute, the actual tempo of the piece, and a wide range of much lower-level beat strengths. This indicates that, as might be expected, ’Round Midnight is more rhythmically complex and is also performed more tightly.</p>
<p><img src="I_Wanna_Be_Sedated.gif"></p>
<p><img src="Round_Midnight.gif"></p>
<p>This type of information can be very representative of different musical classes, such as genre. Techno, for example, often has very clearly defined beats, without any surrounding spread, because the beats are precisely generated electronically. Much modern Classical music, to provide a contrasting example, often has much less clearly defined beats.</p>
<p>The jSymbolic feature catalogue includes the following rhythm-related features:</p>
<ul>
  <li><strong>R-1 Beat Histogram:</strong> A feature vector consisting of the bin magnitudes of the beat histogram described above. The first 40 bins are not included in this feature vector, however. Each bin corresponds to a different beats per minute periodicity, with tempo increasing with the bin index. The magnitude of each bin is proportional to the cumulative loudness (MIDI velocity) of the notes that occur at that bin's rhythmic periodicity. The histogram is normalized.</li>
  <li><strong>R-2 Strongest Rhythmic Pulse:</strong> Bin index of the beat histogram bin with the highest magnitude.</li>
  <li><strong>R-3 Second Strongest Rhythmic Pulse:</strong> Bin index of the beat histogram peak with the second highest magnitude.</li>
  <li><strong>R-4 Harmonicity of Two Strongest Rhythmic Pulses:</strong> Bin index of the higher (in terms of bin index) of the two beat histogram peaks with the highest magnitude, divided by the index of the lower (in terms of bin index) of the two bins.</li>
  <li><strong>R-5 Strength of Strongest Rhythmic Pulse:</strong> Magnitude of the beat histogram bin with the highest magnitude.</li>
  <li><strong>R-6 Strength of Second Strongest Rhythmic Pulse:</strong> Magnitude of the beat histogram peak with the second highest magnitude.</li>
  <li><strong>R-7 Strength Ratio of Two Strongest Rhythmic Pulses:</strong> Magnitude of the higher (in terms of magnitude) of the two beat histogram bins corresponding to the peaks with the two highest magnitudes, divided by the magnitude of the lower peak.</li>
  <li><strong>R-8 Combined Strength of Two Strongest Rhythmic Pulses: </strong>Sum of the magnitudes of the two beat histogram peaks with the highest magnitudes.</li>
  <li><strong>R-9 Number of Strong Rhythmic Pulses:</strong> Number of beat histogram peaks with normalized magnitudes over 0.1.</li>
  <li><strong>R-10 Number of Moderate Rhythmic Pulses:</strong> Number of beat histogram peaks with normalized magnitudes over 0.01.</li>
  <li><strong>R-11 Number of Relatively Strong Rhythmic Pulses:</strong> Number of beat histogram peaks with magnitudes at least 30% as high as the magnitude of the beat histogram peak with the highest magnitude.</li>
  <li><strong>R-12 Rhythmic Looseness:</strong> Average width of beat histogram peaks. The width of a peak is defined here as the distance (in beats per minute) between the two points on the peak in question that have magnitudes closest to 30% of the height of the peak. Only peaks with magnitudes at least 30% as high as the highest peak are considered in this calculation. </li>
  <li><strong>R-13 Polyrhythms:</strong> Number of beat histogram peaks with magnitudes at least 30% as high as the magnitude of the highest peak, and whose bin labels are not integer multiples or factors (using only multipliers of 1, 2, 3, 4, 6 and 8, and with an accepted error of +/- 3 bins) of the bin label of the peak with the highest magnitude. This number is then divided by the total number of bins with frequencies over 30% of the highest magnitude.</li>
  <li><strong>R-14 Rhythmic Variability:</strong> Standard deviation of the beat histogram bin magnitudes.</li>
  <li><strong>R-15 Note Density:</strong> Average number of notes per second.</li>
  <li><strong>	R-16 Note Density Variability:</strong> How much the note density (average number of notes per second) varies throughout the piece. In order to calculate this, the piece is broken into windows of 5 second duration, and the note density of each window is calculated. The final value of this feature is then found by calculating the standard deviation of the note densities of  these windows. Set to 0 if there is insufficient music for more than one window.</li>
  <li><strong>R-17 Average Note Duration:</strong> Average duration of notes (in seconds).</li>
  <li><strong>R-18 Variability of Note Durations:</strong> Standard deviation of note durations (in seconds).</li>
  <li><strong>R-19 Maximum Note Duration: </strong>Duration of the longest note in the piece (in seconds).</li>
  <li><strong>R-20 Minimum Note Duration:</strong> Duration of the shortest note in the piece (in seconds).  Set to 0 if there are no notes.</li>
  <li><strong>R-21 Amount of Staccato:</strong> Number of notes with a duration less than 0.1 seconds, divided by the total number of notes in the piece.</li>
  <li><strong>R-22 Average Time Between Attacks:</strong> Average time (in seconds) between Note On events (regardless of MIDI channel). Set to 0 if there are less than two attacks.</li>
  <li><strong>R-23 Variability of Time Between Attacks:</strong> Standard deviation of the times (in seconds) between Note On events (regardless of MIDI channel).</li>
  <li><strong>R-24 Average Time Between Attacks for Each Voice:</strong> Average of the individual channel averages of time (in seconds) between Note On events in each given MIDI channel. Only channels that contain at least one note are included in this calculation.</li>
  <li><strong>R-25 Average Variability of Time Between Attacks for Each Voice:</strong> Average of the standard deviations (in seconds) of each individual MIDI channel's time between Note On events. Only channels that contain at least one note are included in this calculation.</li>
  <li><strong>R-26 Complete Rests:</strong> Total amount of time (in seconds) in which no pitched notes are sounding on any MIDI channel, divided by the total duration of the piece. Non-pitched (MIDI channel 10) notes are not considered in this calculation.</li>
  <li><strong>R-27 Longest Complete Rest:</strong> Longest amount of uninterrupted time (in seconds) in which no pitched notes are sounding on any MIDI channel. Non-pitched (MIDI channel 10) notes are not considered in this calculation.</li>
  <li><strong>R-28 Average  Rest Fraction Per Voice:</strong> Average of the total amount of combined time (in seconds) in each MIDI channel in which no note is sounding in that particular channel, averaged across channels, and then divided by the total duration of the piece. Only channels with at least one note are counted in this calculation.</li>
  <li><strong>R-29 Variability Across Voices of Total Rests Per Voice:</strong> Standard deviation (in seconds) of the total amount of time per MIDI channel in which no notes are sounding in that channel. Only channels with at least one note are counted in this calculation.</li>
  <li><strong>R-30 Initial Tempo:</strong> Tempo in beats per minute at the start of the piece.</li>
  <li><strong>R-31 Initial Time Signature:</strong> A feature vector consisting of two values. The first is the numerator of the first specified time signature in the piece, and the second is the denominator of the same time signature. Both values are set to 0 if no time signature is specified.</li>
  <li><strong>R-32 Compound or Simple Meter:</strong> Set to 1 if the initial meter is compound (numerator of time signature is greater than or equal to 6 and is evenly divisible by 3) and to 0 if it is simple (if the above condition is not fulfilled).</li>
  <li><strong>R-33 Triple Meter: </strong>Set to 1 if numerator of initial time signature is 3, set to 0 otherwise.</li>
  <li><strong>R-34 Quintuple Meter:</strong> Set to 1 if numerator of initial time signature is 5, set to 0 otherwise. </li>
  <li><strong>R-35 Metrical Diversity:</strong> Set to 1 if the time signature is changed one or more times during the piece.</li>
  <li><strong>R-36 Duration:</strong> The total duration (in seconds) of the piece.</li>
</ul>
<p><strong>FEATURES BASED ON INSTRUMENTATION</strong></p>
<p>Although there is a significant amount of literature on instrumentation with respect to composing and arranging, very few music analytical systems take instrumentation into consideration. This is a shame, as information on instrumentation can in fact be very helpful in discriminating between certain types of musical classes. </p>
<p>As was the case with the feature types presented above, histograms are also useful both as feature vectors and as intermediate data structures for calculating other features. Five instrumentation histograms in particular are used: the <strong>Pitched Instruments Present</strong>, <strong>Unpitched Instruments Present</strong>, <strong>Note Prevalence of Pitched Instruments</strong>, <strong>Note Prevalence of Unpitched Instruments</strong> and <strong>Time Prevalence of Pitched Instruments</strong> histograms. Each of these is described below.</p>

<p>The jSymbolic software capitalizes on the fact that the General MIDI (level 1) specification allows MIDI files to include 128 different pitched-instrument patches, and the MIDI Percussion Key Map permits a further 47 percussion instruments. Although these MIDI instruments are certainly much fewer in number than the full range of extant instruments, particularly with respect to non-Western musics, they are nonetheless diverse enough for a reasonable variety of musical types.</p>

<p>MIDI instrumentation notation can be somewhat sensitive to encoding inconsistencies between different MIDI authors in some cases. In a few fortunately rare cases, authors fail to specify patch numbers, with the result that all notes are played using a piano patch by default. Another problem is the inconsistency in the choice of patches that are used to represent sung lines, since there is no good General MIDI patch for solo vocal lines. Similarly, there are some inconsistencies in MEI with respect to how various instruments are specified.</p>

<p>Despite these occasional problems, however, features based on instrumentation can still be highly characteristic of various musical categories, and the complementary use of other types of features can help to counterbalance inconsistencies in individual authors’ choices of patches.</p>

<p>The jSymbolic feature catalogue includes the following instrumentation-related features:</p>

<ul>
  <li><strong>I-1 Pitched Instruments Present:</strong> A feature vector indicating which pitched instruments are present. Has one entry for each of the 128 General MIDI Instrument patches (0 is Acoustic Piano, 40 is Violin, etc.). Each value is set to 1 if at least one note is played using the corresponding patch, or to 0 if that patch is never used.</li>
  <li><strong>I-2 Unpitched Instruments Present:</strong> A feature vector indicating which unpitched instruments are present. Has one entry for each of the 47 MIDI Percussion Key Map instruments. Each value is set to 1 if at least one note is played using the corresponding instrument, or to 0 if that instrument is never used. It should be noted that only MIDI Channel 10 instruments 35 to 81 are included here, as they are the ones that meet the official standard (they are correspondingly indexed in this feature vector from 0 to 46, such that index 0 corresponds to Acoustic Bass Drum, index 4 corresponds to Hand Clap, etc.).</li>
  <li><strong>I-3 Note Prevalence of Pitched Instruments:</strong> A feature vector indicating the fraction of (pitched) notes played with each of the 128 General MIDI Instrument patches (0 is Acoustic Piano, 40 is Violin, etc.). Has one entry for each of these instruments, and the value of each is set to the number of Note Ons played with the corresponding MIDI patch, divided by the total number of Note Ons in the piece.</li>
  <li><strong>I-4 Note Prevalence of Unpitched Instruments: </strong>A feature vector indicating the fraction of (unpitched) notes played with each of the 47 MIDI Percussion Key Map instruments. Has one entry for each of these 47 instruments, and the value of each is set to the number of Note Ons played with the corresponding instrument, divided by the total number of Note Ons in the piece. It should be noted that only MIDI Channel 10 instruments 35 to 81 are included here, as they are the ones that meet the official standard (they are correspondingly indexed in this feature vector from 0 to 46, such that index 0 corresponds to Acoustic Bass Drum, index 4 corresponds to Hand Clap, etc.).</li>
  <li><strong>I-5 Time Prevalence of Pitched Instruments:</strong> A feature vector indicating the fraction of time during which  (pitched) notes are being sounded by each of the 128 General MIDI Instrument patches (0 is Acoustic Piano, 40 is Violin, etc.). Has one entry for each of these instruments, and the value of each is set to  to the total time in seconds in a piece during which at least one note is being sounded with the corresponding MIDI patch, divided by the total length of the piece in seconds.</li>
  <li><strong>I-6 Variability of Note Prevalence of Pitched Instruments:</strong> Standard deviation of the fraction of total notes in the piece played by each (pitched) General MIDI Instrument patch that is used to play at least one note.</li>
  <li><strong>I-7 Variability of Note Prevalence of Unpitched Instruments:</strong> Standard deviation of the fraction of total notes in the piece played by each (unpitched) MIDI Percussion Key Map instrument that is used to play at least one note. It should be noted that only MIDI Channel 10 instruments 35 to 81 are included here, as they are the ones that meet the official standard.</li>
  <li><strong>I-8 Number of Pitched Instruments:</strong> Total number of (pitched) General MIDI instrument patches that are used to play at least one note.</li>
  <li><strong>I-9 Number of Unpitched Instruments:</strong> Total number of (unpitched) MIDI Percussion Key Map instruments that are used to play at least one note.  It should be noted that only MIDI Channel 10 instruments 35 to 81 are included here, as they are the ones that meet the official standard.</li>
  <li><strong>I-10 Percussion Instrument Prevalence:</strong> Fraction of  all Note Ons played by (unpitched) MIDI Percussion Key Map instruments. It should be noted that only MIDI Channel 10 instruments 35 to 81 are included here, as they are the ones that meet the official standard.</li>
  <li><strong>I-11 String Keyboard Prevalence:</strong> Fraction of all Note Ons played by string keyboard instruments (General MIDI patches 1 to 8).</li>
  <li><strong>I-12 Acoustic Guitar Prevalence:</strong> Fraction of all Note Ons played by acoustic guitar instruments (General MIDI patches 25 and 26).</li>
  <li><strong>I-13 Electric Guitar Prevalence:</strong> Fraction of all Note Ons played by electric guitar instruments (General MIDI patches 27 to 32).</li>
  <li><strong>I-14 Violin Prevalence:</strong> Fraction of all Note Ons played by violin instruments (General MIDI patches 41 or 111).</li>
  <li><strong>I-15 Saxophone Prevalence:</strong> Fraction of all Note Ons played by saxophone instruments (General MIDI patches 65 to 68).</li>
  <li><strong>I-16 Brass Prevalence:</strong> Fraction of all Note Ons played by brass instruments, including saxophones (General MIDI patches 57 to 68).</li>
  <li><strong>I-17 Woodwinds Prevalence:</strong> Fraction of all Note Ons played by woodwind instruments (General MIDI patches 69 to 76).</li>
  <li><strong>I-18 Orchestral Strings Prevalence:</strong> Fraction of all Note Ons played by orchestral string instruments (General MIDI patches 41 to 47).</li>
  <li><strong>I-19 String Ensemble Prevalence:</strong> Fraction of all Note Ons played by orchestral string ensemble instruments (General MIDI patches 49 to 52).</li>
  <li><strong>I-20 Electric Instrument Prevalence:</strong> Fraction of all Note Ons played by electric non-&quot;synth&quot; instruments (General MIDI patches 5, 6, 17, 19, 27 to 32, 34 to 40).</li>
</ul>

<p><strong>FEATURES BASED ON MUSICAL TEXTURE</strong></p>

<p>Although the term &quot;texture&quot; is associated with several different musical meanings, the features falling into this category of the jSymbolic catalogue relate specifically to the number of independent voices in a piece and how these voices relate to one another.</p>

<p>  jSymbolic takes advantage of the fact that MIDI notes can be assigned to different channels, thus making it possible to segregate the notes belonging to different voices. Although it might seem natural to use MIDI tracks instead of channels to distinguish between voices, since only a maximum of sixteen MIDI channels are available, this is an ineffective approach in practice. Using MIDI tracks would mean that it would be impossible to extract texture-based features from all Type 0 MIDI files, since this format only allow permits a single track to be represented. Even in the case of Type 1 files, which do allow tracks to be specified, it is still not unusual to find all MIDI data saved on a single track in practice. Almost all MIDI files do use different channels for different voices, however, and it is possible to take advantage of Program Change messages to multiplex multiple voices onto a single channel in order to avoid being restricted to only sixteen voices. It was therefore decided to use MIDI channels in order to distinguish between voices rather than MIDI tracks.</p>

<p>This approach is not perfect, as it is possible to use a single channel to hold multiple voices even without regular program change messages. A piano could be used to play a four-voice chorale, for example, with all notes occurring on one MIDI channel. This problem is unavoidable with respect to MIDI data, unfortunately, unless one implements a sophisticated voice partitioning pre-processing module to automatically segregate voices prior to feature extraction, something that is beyond the current scope of this work. Fortunately, this problem does not occur  often in MIDI files.</p>
<p>An astute reader will also note that some of the features below pay special attention to the loudest voice, which is to say to voice with the highest average MIDI velocity. This is an imperfect but often effective method for guessing which voice carries the melody, or is in some other way the most important voice.</p>

<p>The jSymbolic feature catalogue includes the following texture-related features:</p>

<ul>
  <li><strong>T-1 Maximum Number of Independent Voices:</strong> Maximum number of different channels in which notes are sounded simultaneously.</li>
  <li><strong>T-2 Average Number of Independent Voices:</strong> Average number of different channels in which notes are sounded simultaneously. Rests are not included in this calculation.</li>
  <li><strong>T-3 Variability of Number of Independent Voices:</strong> Standard deviation of the number of different channels in which notes are sounded simultaneously at each given moment (MIDI tick). Rests are not included in this calculation.</li>
  <li><strong>T-4 Voice Equality – Number of Notes:</strong> Standard deviation of the total number of Note Ons in each channel that contains at least one note.</li>
  <li><strong>T-5 Voice Equality – Note Duration:</strong> Standard deviation of the cumulative amount of time during which one or more notes were sounding in each channel that contains at least one note.</li>
  <li><strong>T-6 Voice Equality – Dynamics:</strong> Standard deviation of the average loudness (MIDI velocity) of notes in each channel that contains at least one note.</li>
  <li><strong>T-7 Voice Equality – Melodic Leaps:</strong> Standard deviation of the average melodic leap distance of each channel that contains at least one note.</li>
  <li><strong>T-8 Voice Equality – Range:</strong> Standard deviation of the differences between the highest and lowest pitches in each channel that contains at least one note.</li>
  <li><strong>T-9 Importance of Loudest Voice:</strong> Difference between the average loudness (MIDI velocity) of the loudest channel and the average loudness of the other channels that contain at least one note.</li>
  <li><strong>T-10 Relative Range of Loudest Voice:</strong> Difference between the highest note and the lowest note played in the channel with the highest average loudness (MIDI velocity), divided by the difference between the highest note and the lowest note in the piece as a whole. Set to 0 if there if there are fewer than 2 pitches in the music.</li>
  <li><strong>T-11 Relative Range Isolation of Loudest Voice:</strong> Number of pitched notes in the MIDI channel with the highest average loudness that fall outside the range of any other pitched channel, divided by the total number of notes in the channel with the highest average loudness. Set to 0 if there are only 0 or 1 channels containing pitched notes.</li>
  <li><strong>T-12 Relative Range of Highest Line:</strong> Pitch difference in semitones between the highest note and the lowest note played in the channel with the highest average pitch, divided by the difference between the highest note and the lowest note in the piece overall. Set to 0 if there if there are fewer than 2 pitches in the music.</li>
  <li><strong>T-13 Relative Note Density of Highest Line:</strong> Number of Note Ons in the channel with the highest average pitch, divided by the average number of Note Ons in all channels that contain at least one note.</li>
  <li><strong>T-14 Relative Note Durations of Lowest Line:</strong> Average duration of notes (in seconds) in the channel with the lowest average pitch, divided by the average duration of notes in all channels that contain at least one note.</li>
  <li><strong>T-15 Relative Size of Melodic Intervals in Lowest Line:</strong> Average melodic interval in semitones in the channel with the lowest average pitch, divided by the average melodic interval in all channels that contain at least two notes.</li>
  <li><strong>T-16 Voice Overlap:</strong> Number of notes played within the range of another channel, divided by the total number of notes in the piece as a whole.</li>
  <li><strong>T-17 Similar Motion:</strong> Fraction of all pitched notes that move together in the same direction within 10% of the duration of the shorter note. If multiple notes are sounding within a single channel (e.g. a piano chord), only the highest pitched note in each such channel is considered for the purposes of this feature. Set to 0 if there are no pitched notes.</li>
  <li><strong>T-18 Voice Separation:</strong> Average separation in semitones between the average pitches of consecutive channels (after sorting based on average pitch) that contain at least one note.</li>
  <li><strong>T-19 Variability of Voice Separation:</strong> Average separation in semi-tones between the average pitches of consecutive channels (after sorting based on average pitch) that contain at least one note.</li>
</ul>

<p><strong>FEATURES BASED ON DYNAMICS</strong></p>

<p>The ways in which musical dynamics are used in a piece can also be characteristic of different types of musical classes. Once again, however, this information is only rarely used in traditional analytical systems, and is generally notated only very coarsely in musical scores. Fortunately, MIDI velocity values make it possible to annotate dynamics much more precisely, even though MIDI encodings generated by score editing software admittedly generally fail to take full advantage of this.</p>

<p>One important point to consider with respect to MIDI dynamics is that, while MIDI velocities are generally used to indicate the strength with which notes are sounded, this is not the only way in which loudness is controlled. MIDI channel volume can also be changed independently. jSymbolic takes this into account by using the following formula to find loudness values used to calculate the features described in this sub-section:</p>

<p>loudness = note velocity x (channel volume / 127)</p>

<p>It should also be noted that all of the jSymbolic features related to dynamics use relative measures of loudness rather than absolute measures because the default volume and velocity values set by sequencers can vary.</p>

<p>The jSymbolic feature catalogue includes the following features related to dynamics:</p>

<ul>
  <li><strong>D-1 Dynamic Range: </strong>Loudness of the loudest note in the piece, minus the loudness of the softest note.</li>
  <li><strong>D-2 Variation of Dynamics: </strong>Standard deviation of  loudness levels across all notes.</li>
  <li><strong>D-3 Variation of Dynamics in Each Voice:</strong> Standard deviations of note loudness levels within each MIDI channel, averaged across all channels. Only channels that contain at least one note are included in this calculation.</li>
  <li><strong>D-4 Average Note to Note  Change in Dynamics:</strong> Average change of loudness from one note to the next note in the same MIDI channel.
  </li>
</ul>

<p><strong>MEI-SPECIFIC FEATURES</strong></p>

<p>There is certain information that cannot be encoded in MIDI files, due to limitations of the MIDI format. jSymbolic provides access to this data through a separate data pipeline, which can be accessed by MEI-specific features, such as the sample features listed below.</p>
<p>MEI-specific features rely on information that is available in MEI files, but cannot be encoded in MIDI files. These features should  only be used when extracting features from a musical collection consisting exclusively of MEI files, as a feature set extracted from a mixture of MIDI and MEI files would be inconsistent if MEI-specific features are made available for MEI instances but not for MIDI instances.</p>
<p>The jSymbolic feature catalogue includes the following MEI-specific features:</p>
<ul>
  <li><strong>S-1 Number of Grace Notes:</strong> The total number of grace notes in a piece (i.e. the number of notes indicated as grace notes in the MEI encoding) divided by the total number of pitched notes in the music.</li>
  <li><strong>S-2 Number of Slurs:</strong> The total number of notes marked with slurs in the piece (i.e. the number of notes indicated as notes with slurs in the MEI encoding) divided by the total number of pitched notes in the music.</li>
</ul>
<p></p>
<table height="5" width="100%" bgcolor="#0033C4" border="0"><tbody><tr><th></th></tr></tbody></table>
<p><tt><a href="../featureexplanations_files/featureexplanations.html#Top">-top of page-</a></tt></p>

</body></html>